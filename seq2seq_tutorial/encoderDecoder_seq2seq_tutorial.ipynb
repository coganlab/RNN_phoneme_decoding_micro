{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (seq2seq) LSTM Encoder-Decoder Model Tutorial\n",
    "\n",
    "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Average, Concatenate, Bidirectional, TimeDistributed, Dense, Dropout, LSTM, Activation, Multiply, Conv1D, Conv3D\n",
    "from keras.regularizers import L2\n",
    "from keras.utils import to_categorical\n",
    "from keras_tuner import RandomSearch, BayesianOptimization, Objective\n",
    "import keras_tuner as kt\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_model(n_input, n_output, n_units):\n",
    "    \"\"\"Creates a LSTM encoder-decoder model for sequence-to-sequence prediction in Keras.\n",
    "\n",
    "    Args:\n",
    "        n_input (int): The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n",
    "        n_output (int): The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n",
    "        n_units (int): The number of cells to create in the encoder and decoder models, e.g. 128 or 256.\n",
    "\n",
    "    Returns:\n",
    "        (Model, Model, Model): A tuple of Keras models for training, inference encoder, and inference decoder.\n",
    "    \"\"\"    \n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax') # proability distribution across ouptut classes\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # training model, takes input data and training sequences\n",
    "\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequences(infenc, infdec, source, n_steps, cardinality):\n",
    "    \"\"\"Predict the target sequence for a given source sequence using inference encoder and decoder models.\n",
    "\n",
    "    Args:\n",
    "        infenc (keras.models Model): Encoder model used when making a prediction for a new source sequence.\n",
    "        infdec (keras.models Model): Decoder model use when making a prediction for a new source sequence.\n",
    "        source (array-like): Source sequence.\n",
    "        n_steps (int): Length of target sequence.\n",
    "        cardinality (int): The cardinality of each output sequence element, i.e. number of classes to predict sequence elements from.\n",
    "    \"\"\"    \n",
    "    # encode source through encoder model\n",
    "    state = infenc.predict(source)\n",
    "    # initial decoder sequence input\n",
    "    target_dist = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "    output = []\n",
    "    for _ in range(n_steps):\n",
    "        yhat, h, c = infdec.predict([target_dist] + state)\n",
    "        # save prediction distribution\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update states\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_dist = yhat\n",
    "        return np.array(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq Data Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(length, n_unique):\n",
    "    \"\"\"Generates a sequence of random integers in the range 1 to n_unique.\n",
    "\n",
    "    Args:\n",
    "        length (int): length of sequence to generate.\n",
    "        n_unique (int): cardinality of input sequence elements (upper bound on random integers to generate).\n",
    "\n",
    "    Returns:\n",
    "        array-like: sequence of random integers in range 1 to n_unique.\n",
    "    \"\"\"\n",
    "    return [random.randint(1, n_unique - 1) for _ in range(length)]\n",
    "\n",
    "def reverse_sequence_first_n(source, n_out):\n",
    "    \"\"\"Reverses the first n elements of a source sequence and returns only the reversed sequence.\n",
    "\n",
    "    Args:\n",
    "        source (array-like): sequence to reverse.\n",
    "        n_out (int): number of elements in source to reverse.\n",
    "\n",
    "    Returns:\n",
    "        array-like: first n elements of source sequence in reverse order.\n",
    "    \"\"\"      \n",
    "    return source[:n_out][::-1]\n",
    "\n",
    "def pad_sequence(source):\n",
    "    \"\"\"Shifts a source sequence by one element and pads the first element with a 0.\n",
    "\n",
    "    Args:\n",
    "        source (array-like): Sequence to shift and pad.\n",
    "\n",
    "    Returns:\n",
    "        array-like: Shifted and padded sequence.\n",
    "    \"\"\"    \n",
    "    return [0] + source[:-1]\n",
    "\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "    \"\"\"Generates a dataset of source, target, and shifted target sequences.\n",
    "\n",
    "    Args:\n",
    "        n_in (int): Length of source sequences.\n",
    "        n_out (int): Length of target sequences.\n",
    "        cardinality (int): Cardinality of source and target sequences.\n",
    "        n_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        (array-like, array-like, array-like): Tuple of source, target, and shifted target sequences.\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for _ in range(n_samples):\n",
    "        # create source sequence\n",
    "        source = generate_sequence(n_in, cardinality)\n",
    "\n",
    "        # create target sequence from source\n",
    "        target = reverse_sequence_first_n(source, n_out)\n",
    "        target_pad = pad_sequence(target)\n",
    "\n",
    "        # encode source data for model input\n",
    "        src_encoded = to_categorical(source, num_classes=cardinality)\n",
    "        tar_encoded = to_categorical(target, num_classes=cardinality)\n",
    "        tar_pad_encoded = to_categorical(target_pad, num_classes=cardinality)\n",
    "\n",
    "        # save data for current sample\n",
    "        X1.append(src_encoded)\n",
    "        X2.append(tar_pad_encoded)\n",
    "        y.append(tar_encoded)\n",
    "\n",
    "        return np.array(X1), np.array(X2), np.array(y)\n",
    "    \n",
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"Decodes a class from a one hot encoded sequence.\n",
    "\n",
    "    Args:\n",
    "        encoded_seq (array-like): One hot encoded sequence.\n",
    "\n",
    "    Returns:\n",
    "        list: Decoded sequence.\n",
    "    \"\"\"\n",
    "    return [np.argmax(class_dist) for class_dist in encoded_seq]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sequence Shape: (1, 6, 51)\n",
      "Padded Target Sequence Shape: (1, 3, 51)\n",
      "Target Sequence Shape: (1, 3, 51)\n",
      "\n",
      "Source Sequence Sample: [2, 12, 29, 1, 47, 23]\n",
      "Padded Target Sequence Sample: [0, 29, 12]\n",
      "Target Sequence Sample: [29, 12, 2]\n"
     ]
    }
   ],
   "source": [
    "n_feats = 50 + 1 # 50 features + 1 value reserved for padding (input to decoder model for prediction of first element)\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_feats, 1)\n",
    "print(f'Source Sequence Shape: {X1.shape}')\n",
    "print(f'Padded Target Sequence Shape: {X2.shape}')\n",
    "print(f'Target Sequence Shape: {y.shape}\\n')\n",
    "\n",
    "print(f'Source Sequence Sample: {one_hot_decode(X1[0])}')\n",
    "print(f'Padded Target Sequence Sample: {one_hot_decode(X2[0])}')\n",
    "print(f'Target Sequence Sample: {one_hot_decode(y[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70ea75fe4d4912c04c5b3d05361445264347b81ab4a9aec88659d0b476cbe73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
