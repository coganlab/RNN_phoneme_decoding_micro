{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from processing_utils.feature_data_from_mat import load_subject_high_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = True\n",
    "zscore = False\n",
    "cluster = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phon_labels = ['a', 'ae', 'i', 'u', 'b', 'p', 'v', 'g', 'k']\n",
    "artic_labels = ['low', 'high', 'labial', 'dorsal']\n",
    "phon_to_artic_dict = {1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:3, 8:4, 9:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phon_to_artic(phon_idx, phon_to_artic_conv):\n",
    "    return phon_to_artic_conv[phon_idx]\n",
    "\n",
    "def phon_to_artic_seq(phon_seq, phon_to_artic_conv):\n",
    "    flat_seq = phon_seq.flatten()\n",
    "    artic_conv = np.array([phon_to_artic(phon_idx, phon_to_artic_conv) for phon_idx in flat_seq])\n",
    "    return np.reshape(artic_conv, phon_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative to Response Onset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in S14 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_hg_trace, S14_hg_map, S14_phon_labels = load_subject_high_gamma('S14', sig_channel=sig, zscore=zscore, cluster=cluster, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S14_hg_trace.shape)\n",
    "print(S14_hg_map.shape)\n",
    "print(S14_phon_labels.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(-0.5, 0.5, 200), np.mean(S14_hg_trace, axis=2).T, 'grey', alpha=0.35)\n",
    "plt.plot(np.linspace(-0.5, 0.5, 200), np.mean(np.mean(S14_hg_trace, axis=2), axis=0), 'black')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('HG (Mean-subtracted)')\n",
    "plt.title('S14 HG Trace by Trial')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in S26 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S26_hg_trace, S26_hg_map, S26_phon_labels = load_subject_high_gamma('S26', sig_channel=sig, zscore=zscore, cluster=cluster, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S26_hg_trace.shape)\n",
    "print(S26_hg_map.shape)\n",
    "print(S26_phon_labels.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.mean(S26_hg_trace, axis=0), 'grey')\n",
    "plt.plot(np.mean(np.mean(S26_hg_trace, axis=0), axis=1), 'black')\n",
    "plt.title('S26 HG Trace by Channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in S23 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S23_hg_trace, S23_hg_map, S23_phon_labels = load_subject_high_gamma('S23', sig_channel=sig, zscore=zscore, cluster=cluster, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S23_hg_trace.shape)\n",
    "print(S23_hg_map.shape)\n",
    "print(S23_phon_labels.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.mean(S23_hg_trace, axis=0), 'grey')\n",
    "plt.plot(np.mean(np.mean(S23_hg_trace, axis=0), axis=1), 'black')\n",
    "plt.title('S23 HG Trace by Channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in S33 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S33_hg_trace, S33_hg_map, S33_phon_labels = load_subject_high_gamma('S33', sig_channel=sig, zscore=zscore, cluster=cluster, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S33_hg_trace.shape)\n",
    "print(S33_hg_map.shape)\n",
    "print(S33_phon_labels.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.mean(S33_hg_trace, axis=0), 'grey')\n",
    "plt.plot(np.mean(np.mean(S33_hg_trace, axis=0), axis=1), 'black')\n",
    "plt.title('S33 HG Trace by Channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative to Different Phoneme Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_utils.feature_data_from_mat import load_subject_high_gamma_phoneme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S14 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_hg_data = load_subject_high_gamma_phoneme('S14', phons=[1, 2, 3], cluster=cluster, zscore=zscore, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S14_hg_data['X1'].shape, S14_hg_data['X2'].shape, S14_hg_data['X3'].shape)\n",
    "print(S14_hg_data['X1_map'].shape, S14_hg_data['X2_map'].shape, S14_hg_data['X3_map'].shape)\n",
    "print(S14_hg_data['y1'].shape, S14_hg_data['y2'].shape, S14_hg_data['y3'].shape, S14_hg_data['y_full_phon'].shape)\n",
    "\n",
    "t = np.linspace(-0.5, 0.5, S14_hg_data['X1'].shape[1])\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(18, 5))\n",
    "ax1.plot(t, np.mean(S14_hg_data['X1'], axis=0), 'grey')\n",
    "ax1.plot(t, np.mean(np.mean(S14_hg_data['X1'], axis=0), axis=1), 'black')\n",
    "ax1.set_title('P1')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('High Gamma')\n",
    "\n",
    "ax2.plot(t, np.mean(S14_hg_data['X2'], axis=0), 'grey')\n",
    "ax2.plot(t, np.mean(np.mean(S14_hg_data['X2'], axis=0), axis=1), 'black')\n",
    "ax2.set_title('P2')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "# ax2.set_ylabel('High Gamma')\n",
    "\n",
    "ax3.plot(t, np.mean(S14_hg_data['X3'], axis=0), 'grey')\n",
    "ax3.plot(t, np.mean(np.mean(S14_hg_data['X3'], axis=0), axis=1), 'black')\n",
    "ax3.set_title('P3')\n",
    "ax3.set_xlabel('Time (s)')\n",
    "# ax3.set_ylabel('High Gamma')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S26 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S26_hg_data = load_subject_high_gamma_phoneme('S26', phons=[1, 2, 3], cluster=cluster, zscore=zscore, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S26_hg_data['X1'].shape, S26_hg_data['X2'].shape, S26_hg_data['X3'].shape)\n",
    "print(S26_hg_data['X1_map'].shape, S26_hg_data['X2_map'].shape, S26_hg_data['X3_map'].shape)\n",
    "print(S26_hg_data['y1'].shape, S26_hg_data['y2'].shape, S26_hg_data['y3'].shape, S26_hg_data['y_full_phon'].shape)\n",
    "\n",
    "t = np.linspace(-0.5, 0.5, S26_hg_data['X1'].shape[1])\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(18, 5))\n",
    "ax1.plot(t, np.mean(S26_hg_data['X1'], axis=0), 'grey')\n",
    "ax1.plot(t, np.mean(np.mean(S26_hg_data['X1'], axis=0), axis=1), 'black')\n",
    "ax1.set_title('P1')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('High Gamma')\n",
    "\n",
    "ax2.plot(t, np.mean(S26_hg_data['X2'], axis=0), 'grey')\n",
    "ax2.plot(t, np.mean(np.mean(S26_hg_data['X2'], axis=0), axis=1), 'black')\n",
    "ax2.set_title('P2')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "# ax2.set_ylabel('High Gamma')\n",
    "\n",
    "ax3.plot(t, np.mean(S26_hg_data['X3'], axis=0), 'grey')\n",
    "ax3.plot(t, np.mean(np.mean(S26_hg_data['X3'], axis=0), axis=1), 'black')\n",
    "ax3.set_title('P3')\n",
    "ax3.set_xlabel('Time (s)')\n",
    "# ax3.set_ylabel('High Gamma')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S23 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S23_hg_data = load_subject_high_gamma_phoneme('S23', phons=[1, 2, 3], cluster=cluster, zscore=zscore, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S23_hg_data['X1'].shape, S23_hg_data['X2'].shape, S23_hg_data['X3'].shape)\n",
    "print(S23_hg_data['X1_map'].shape, S23_hg_data['X2_map'].shape, S23_hg_data['X3_map'].shape)\n",
    "print(S23_hg_data['y1'].shape, S23_hg_data['y2'].shape, S23_hg_data['y3'].shape, S23_hg_data['y_full_phon'].shape)\n",
    "\n",
    "t = np.linspace(-0.5, 0.5, S23_hg_data['X1'].shape[1])\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(18, 5))\n",
    "ax1.plot(t, np.mean(S23_hg_data['X1'], axis=0), 'grey')\n",
    "ax1.plot(t, np.mean(np.mean(S23_hg_data['X1'], axis=0), axis=1), 'black')\n",
    "ax1.set_title('P1')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('High Gamma')\n",
    "\n",
    "ax2.plot(t, np.mean(S23_hg_data['X2'], axis=0), 'grey')\n",
    "ax2.plot(t, np.mean(np.mean(S23_hg_data['X2'], axis=0), axis=1), 'black')\n",
    "ax2.set_title('P2')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "# ax2.set_ylabel('High Gamma')\n",
    "\n",
    "ax3.plot(t, np.mean(S23_hg_data['X3'], axis=0), 'grey')\n",
    "ax3.plot(t, np.mean(np.mean(S23_hg_data['X3'], axis=0), axis=1), 'black')\n",
    "ax3.set_title('P3')\n",
    "ax3.set_xlabel('Time (s)')\n",
    "# ax3.set_ylabel('High Gamma')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S33 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S33_hg_data = load_subject_high_gamma_phoneme('S33', phons=[1, 2, 3], cluster=cluster, zscore=zscore, data_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S33_hg_data['X1'].shape, S33_hg_data['X2'].shape, S33_hg_data['X3'].shape)\n",
    "print(S33_hg_data['X1_map'].shape, S33_hg_data['X2_map'].shape, S33_hg_data['X3_map'].shape)\n",
    "print(S33_hg_data['y1'].shape, S33_hg_data['y2'].shape, S33_hg_data['y3'].shape, S33_hg_data['y_full_phon'].shape)\n",
    "\n",
    "t = np.linspace(-0.5, 0.5, S33_hg_data['X1'].shape[1])\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(18, 5))\n",
    "ax1.plot(t, np.mean(S33_hg_data['X1'], axis=0), 'grey')\n",
    "ax1.plot(t, np.mean(np.mean(S33_hg_data['X1'], axis=0), axis=1), 'black')\n",
    "ax1.set_title('P1')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('High Gamma')\n",
    "\n",
    "ax2.plot(t, np.mean(S33_hg_data['X2'], axis=0), 'grey')\n",
    "ax2.plot(t, np.mean(np.mean(S33_hg_data['X2'], axis=0), axis=1), 'black')\n",
    "ax2.set_title('P2')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "# ax2.set_ylabel('High Gamma')\n",
    "\n",
    "ax3.plot(t, np.mean(S33_hg_data['X3'], axis=0), 'grey')\n",
    "ax3.plot(t, np.mean(np.mean(S33_hg_data['X3'], axis=0), axis=1), 'black')\n",
    "ax3.set_title('P3')\n",
    "ax3.set_xlabel('Time (s)')\n",
    "# ax3.set_ylabel('High Gamma')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_artic_labels = phon_to_artic_seq(S14_phon_labels, phon_to_artic_dict)\n",
    "S26_artic_labels = phon_to_artic_seq(S26_phon_labels, phon_to_artic_dict)\n",
    "S23_artic_labels = phon_to_artic_seq(S23_phon_labels, phon_to_artic_dict)\n",
    "S33_artic_labels = phon_to_artic_seq(S33_phon_labels, phon_to_artic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Across Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_hg_collapsed = np.concatenate((S14_hg_data['X1'], S14_hg_data['X2'], S14_hg_data['X3']), axis=0)\n",
    "S14_phon_labels_collapsed = np.concatenate((S14_hg_data['y1'], S14_hg_data['y2'], S14_hg_data['y3']), axis=0)\n",
    "\n",
    "S26_hg_collapsed = np.concatenate((S26_hg_data['X1'], S26_hg_data['X2'], S26_hg_data['X3']), axis=0)\n",
    "S26_phon_labels_collapsed = np.concatenate((S26_hg_data['y1'], S26_hg_data['y2'], S26_hg_data['y3']), axis=0)\n",
    "\n",
    "S23_hg_collapsed = np.concatenate((S23_hg_data['X1'], S23_hg_data['X2'], S23_hg_data['X3']), axis=0)\n",
    "S23_phon_labels_collapsed = np.concatenate((S23_hg_data['y1'], S23_hg_data['y2'], S23_hg_data['y3']), axis=0)\n",
    "\n",
    "S33_hg_collapsed = np.concatenate((S33_hg_data['X1'], S33_hg_data['X2'], S33_hg_data['X3']), axis=0)\n",
    "S33_phon_labels_collapsed = np.concatenate((S33_hg_data['y1'], S33_hg_data['y2'], S33_hg_data['y3']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_artic_labels_collapsed = phon_to_artic_seq(S14_phon_labels_collapsed, phon_to_artic_dict)\n",
    "S26_artic_labels_collapsed = phon_to_artic_seq(S26_phon_labels_collapsed, phon_to_artic_dict)\n",
    "S23_artic_labels_collapsed = phon_to_artic_seq(S23_phon_labels_collapsed, phon_to_artic_dict)\n",
    "S33_artic_labels_collapsed = phon_to_artic_seq(S33_phon_labels_collapsed, phon_to_artic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint PCA Decomp Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_hg_data['y_full_artic'] = S14_artic_labels\n",
    "S26_hg_data['y_full_artic'] = S26_artic_labels\n",
    "S23_hg_data['y_full_artic'] = S23_artic_labels\n",
    "S33_hg_data['y_full_artic'] = S33_artic_labels\n",
    "\n",
    "S14_hg_data['X_collapsed'] = S14_hg_collapsed\n",
    "S26_hg_data['X_collapsed'] = S26_hg_collapsed\n",
    "S23_hg_data['X_collapsed'] = S23_hg_collapsed\n",
    "S33_hg_data['X_collapsed'] = S33_hg_collapsed\n",
    "\n",
    "S14_hg_data['y_phon_collapsed'] = S14_phon_labels_collapsed\n",
    "S26_hg_data['y_phon_collapsed'] = S26_phon_labels_collapsed\n",
    "S23_hg_data['y_phon_collapsed'] = S23_phon_labels_collapsed\n",
    "S33_hg_data['y_phon_collapsed'] = S33_phon_labels_collapsed\n",
    "\n",
    "S14_hg_data['y_artic_collapsed'] = S14_artic_labels_collapsed\n",
    "S26_hg_data['y_artic_collapsed'] = S26_artic_labels_collapsed\n",
    "S23_hg_data['y_artic_collapsed'] = S23_artic_labels_collapsed\n",
    "S33_hg_data['y_artic_collapsed'] = S33_artic_labels_collapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S14_hg_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dict = {}\n",
    "\n",
    "# merge pretrain information with pt data\n",
    "pt_dict['S14'] = S14_hg_data | {'pre_pts': ['S26', 'S23', 'S33']}\n",
    "pt_dict['S26'] = S26_hg_data | {'pre_pts': ['S14', 'S23', 'S33']}\n",
    "pt_dict['S23'] = S23_hg_data | {'pre_pts': ['S14', 'S26', 'S33']}\n",
    "# pt_dict['S33'] = S33_hg_data | {'pre_pts': ['S14', 'S26', 'S23']}\n",
    "pt_dict['S33'] = S33_hg_data | {'pre_pts': ['S14', 'S23', 'S26']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0681f22c18b41739c1b2595661b15c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5058865970630676\n",
      "0.4956702883173471\n",
      "0.4857482386894151\n",
      "0.5282955606485018\n",
      "0.47988144752850637\n",
      "0.524355600826189\n",
      "0.5033655886597063\n",
      "0.5197818521347933\n",
      "0.5271814786520669\n",
      "0.5032163370398666\n",
      "[0.5058865970630676, 0.4956702883173471, 0.4857482386894151, 0.5282955606485018, 0.47988144752850637, 0.524355600826189, 0.5033655886597063, 0.5197818521347933, 0.5271814786520669, 0.5032163370398666]\n",
      "Mean acc: 0.507338298955946, Std: 0.016335303722812205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from alignment_methods import JointPCADecomp, CCAAlign\n",
    "\n",
    "# patient and target params\n",
    "pt = 'S14'\n",
    "p_ind = 1\n",
    "\n",
    "# experiment params\n",
    "pool_pre = True\n",
    "tar_in_train = True\n",
    "cca_algn = True\n",
    "\n",
    "# constant params\n",
    "n_iter = 10\n",
    "n_folds = 5\n",
    "n_comp = 30\n",
    "\n",
    "# alignment label type\n",
    "# algn_type = 'artic_seq'\n",
    "algn_type = 'phon_seq'\n",
    "algn_grouping = 'class'\n",
    "\n",
    "# decoding label type\n",
    "lab_type = 'phon'\n",
    "# lab_type = 'artic'\n",
    "\n",
    "# dimensionality reduction\n",
    "red_method = 'PCA'\n",
    "dim_red = PCA\n",
    "\n",
    "pre_pts = pt_dict[pt]['pre_pts']\n",
    "if p_ind == -1:\n",
    "    D_tar = pt_dict[pt]['X_collapsed']\n",
    "    lab_tar = pt_dict[pt]['y_' + lab_type + '_collapsed']\n",
    "    \n",
    "\n",
    "    D1 = pt_dict[pre_pts[0]]['X_collapsed']\n",
    "    lab1 = pt_dict[pre_pts[0]]['y_' + lab_type + '_collapsed']\n",
    "    \n",
    "\n",
    "    D2 = pt_dict[pre_pts[1]]['X_collapsed']\n",
    "    lab2 = pt_dict[pre_pts[1]]['y_' + lab_type + '_collapsed']\n",
    "    \n",
    "\n",
    "    D3 = pt_dict[pre_pts[2]]['X_collapsed']\n",
    "    lab3 = pt_dict[pre_pts[2]]['y_' + lab_type + '_collapsed']\n",
    "    \n",
    "else:\n",
    "    D_tar = pt_dict[pt]['X' + str(p_ind)]\n",
    "    lab_tar = pt_dict[pt]['y' + str(p_ind)]\n",
    "\n",
    "    D1 = pt_dict[pre_pts[0]]['X' + str(p_ind)]\n",
    "    lab1 = pt_dict[pre_pts[0]]['y' + str(p_ind)]\n",
    "\n",
    "    D2 = pt_dict[pre_pts[1]]['X' + str(p_ind)]\n",
    "    lab2 = pt_dict[pre_pts[1]]['y' + str(p_ind)]\n",
    "\n",
    "    D3 = pt_dict[pre_pts[2]]['X' + str(p_ind)]\n",
    "    lab3 = pt_dict[pre_pts[2]]['y' + str(p_ind)]\n",
    "\n",
    "    if lab_type == 'artic':\n",
    "        lab_tar = phon_to_artic_seq(lab_tar, phon_to_artic_dict)\n",
    "        lab1 = phon_to_artic_seq(lab1, phon_to_artic_dict)\n",
    "        lab2 = phon_to_artic_seq(lab2, phon_to_artic_dict)\n",
    "        lab3 = phon_to_artic_seq(lab3, phon_to_artic_dict)\n",
    "\n",
    "lab_tar_full = pt_dict[pt]['y_full_' + algn_type[:-4]]\n",
    "lab1_full = pt_dict[pre_pts[0]]['y_full_' + algn_type[:-4]]\n",
    "lab2_full = pt_dict[pre_pts[1]]['y_full_' + algn_type[:-4]]\n",
    "lab3_full = pt_dict[pre_pts[2]]['y_full_' + algn_type[:-4]]\n",
    "if p_ind == -1:\n",
    "    lab_tar_full = np.tile(lab_tar_full, (3, 1))\n",
    "    lab1_full = np.tile(lab1_full, (3, 1))\n",
    "    lab2_full = np.tile(lab2_full, (3, 1))\n",
    "    lab3_full = np.tile(lab3_full, (3, 1))\n",
    "\n",
    "iter_accs = []\n",
    "for _ in tqdm(range(n_iter)):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    for train_idx, test_idx in cv.split(D_tar, lab_tar):\n",
    "        X1, X2, X3 = D1, D2, D3\n",
    "        y1, y2, y3 = lab1, lab2, lab3\n",
    "        y1_full, y2_full, y3_full = lab1_full, lab2_full, lab3_full\n",
    "        \n",
    "        # split target data into train and test\n",
    "        X_tar_train, X_tar_test = D_tar[train_idx], D_tar[test_idx]\n",
    "        y_tar_train, y_tar_test = lab_tar[train_idx], lab_tar[test_idx]\n",
    "        y_tar_full_train, y_tar_full_test = (lab_tar_full[train_idx],\n",
    "                                             lab_tar_full[test_idx])\n",
    "\n",
    "        # learn joint PCA decomposition from full articulator sequences\n",
    "        jointPCA = JointPCADecomp(n_components=n_comp)\n",
    "        X1, X2, X3, X_tar_train = jointPCA.fit_transform([X1, X2, X3,\n",
    "                                                          X_tar_train],\n",
    "                                                         [y1_full, y2_full,\n",
    "                                                          y3_full,\n",
    "                                                          y_tar_full_train])\n",
    "        # apply target transformation to test data\n",
    "        X_tar_test = jointPCA.transform(X_tar_test, idx=3)\n",
    "\n",
    "        # X_tar_train_p = X_tar_train.reshape(-1, X_tar_train.shape[-1])\n",
    "        # X_tar_test_p = X_tar_test.reshape(-1, X_tar_test.shape[-1])\n",
    "        # pca = PCA(n_components=n_comp)\n",
    "        # X_tar_train_p = pca.fit_transform(X_tar_train_p)\n",
    "        # X_tar_test_p = pca.transform(X_tar_test_p)\n",
    "        # X_tar_train = X_tar_train_p.reshape(X_tar_train.shape[0], -1, n_comp)\n",
    "        # X_tar_test = X_tar_test_p.reshape(X_tar_test.shape[0], -1, n_comp)\n",
    "\n",
    "        # align each pooled patient data to target data with CCA\n",
    "        if cca_algn:\n",
    "            cca1 = CCAAlign(type=algn_grouping)\n",
    "            cca2 = CCAAlign(type=algn_grouping)\n",
    "            cca3 = CCAAlign(type=algn_grouping)\n",
    "            cca1.fit(X_tar_train, X1, y_tar_full_train, y1_full)\n",
    "            cca2.fit(X_tar_train, X2, y_tar_full_train, y2_full)\n",
    "            cca3.fit(X_tar_train, X3, y_tar_full_train, y3_full)\n",
    "            X1 = cca1.transform(X1)\n",
    "            X2 = cca2.transform(X2)\n",
    "            X3 = cca3.transform(X3)\n",
    "\n",
    "        # reshape to trials x features\n",
    "        X_tar_train = X_tar_train.reshape(X_tar_train.shape[0], -1)\n",
    "        X_tar_test = X_tar_test.reshape(X_tar_test.shape[0], -1)\n",
    "        X1 = X1.reshape(X1.shape[0], -1)\n",
    "        X2 = X2.reshape(X2.shape[0], -1)\n",
    "        X3 = X3.reshape(X3.shape[0], -1)\n",
    "\n",
    "        if not pool_pre:\n",
    "            X_train, y_train = X_tar_train, y_tar_train\n",
    "        else:\n",
    "            if not tar_in_train:\n",
    "                X_train = np.concatenate((X1, X2, X3), axis=0)\n",
    "                y_train = np.concatenate((y1, y2, y3), axis=0)\n",
    "            else:\n",
    "                X_train = np.concatenate((X_tar_train, X1, X2, X3), axis=0)\n",
    "                y_train = np.concatenate((y_tar_train, y1, y2, y3), axis=0)\n",
    "                # X_train = np.concatenate((X_tar_train, X1, X3), axis=0)\n",
    "                # y_train = np.concatenate((y_tar_train, y1, y3), axis=0)\n",
    "        X_test = X_tar_test\n",
    "        y_test = y_tar_test\n",
    "\n",
    "        # sc = MinMaxScaler()\n",
    "        # X_train = sc.fit_transform(X_train)\n",
    "        # X_test = sc.transform(X_test)\n",
    "\n",
    "        clf = BaggingClassifier(base_estimator=SVC(kernel='linear', C=0.5),\n",
    "                                n_estimators=10)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "    iter_acc = balanced_accuracy_score(y_true_all, y_pred_all)\n",
    "    print(iter_acc)\n",
    "    iter_accs.append(iter_acc)\n",
    "\n",
    "print(iter_accs)\n",
    "print(f'Mean acc: {np.mean(iter_accs)}, Std: {np.std(iter_accs)}')\n",
    "print()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
